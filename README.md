# Synthetic Generalisation

I am curious as to how much data is needed for a neural network to learn a complete space. For instance, if a model predicts label1 vs label2 for 4 pixel black-white images, how many of the 2^5 combinations are needed in the trainset to achieve perfect generalization?

# More thoughts
* How does model size and architecture affect generalization
* How does the percentage that is needed to generalize scale for larger spaces
* How are continous spaces handeled? In a computer they are kind of just large discrete spaces...
* How does label amout impact?
* Label distribution?

# Future endevours
* Use embeddings of tasks and agents to simulate all possible combinations or rl settings and let a model learn key insights

# Notes
This is a work-in-progress repository where I order my thoughts and flesh out some code. Discussions are always welcome:)
